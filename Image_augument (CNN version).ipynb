{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "face_cascade = cv2.CascadeClassifier('/Users/jack_wu/opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('/Users/jack_wu/opencv/data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "\n",
    "\n",
    "def image_augument(item_Num, age, gender): #item_Num = count\n",
    "\ts_jpg='.jpg'\n",
    "\tinput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/FinalProject_dataset/child/male/\"\n",
    "\t#input_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/FinalProject_dataset/\"+age+'/'+gender+'/'\n",
    "\t#output_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Image_augument/\"+age+'/'+gender+'/'\n",
    "\toutput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Image_augument/\"\n",
    "\tcount = 0\n",
    "\tfor file_count in range(item_Num):\n",
    "\t\ti = 0\n",
    "\t\ts_file_count = str(file_count)\n",
    "\t\timg = cv2.imread(input_address + s_file_count + s_jpg)\n",
    "\t\tx = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "\t\tx = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\t\tfor batch in datagen.flow(x, batch_size=1, save_to_dir=output_address, save_format='jpg'):\n",
    "\t\t\ti += 1\n",
    "\t\t\tcount +=1\n",
    "\t\t\tif i > 10:\n",
    "\t\t\t\tbreak  # otherwise the generator would loop indefinitely\n",
    "\treturn count\n",
    "\n",
    "def initial_face(item_Num, age, gender):\n",
    "\ts_jpg='.jpg'\n",
    "\tcount = 0\n",
    "\t#All_jpg=glob.glob('/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Image_augument/'+age+'/'+gender+'/'+'*.jpg')\n",
    "\tAll_jpg=glob.glob('/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Image_augument/*.jpg')\n",
    "\t#output_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'\n",
    "\toutput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"\n",
    "\t#for file_count in range(item_Num):\n",
    "\tfor jpg in All_jpg:\n",
    "\t\timg = cv2.imread(jpg)\n",
    "\t\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\t\tif (len(faces)!=0):\n",
    "\t\t\tfor (x,y,w,h) in faces:\n",
    "\t\t\t\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\t\t\t\troi_gray = gray[y:y+h, x:x+w]\n",
    "\t\t\t\tresize_face=cv2.resize(roi_gray,(30,30),interpolation=cv2.INTER_AREA)\n",
    "\t\t\t\tcv2.imwrite(output_address+str(count)+s_jpg, resize_face, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\telse:\n",
    "\t\t\t## No face detect\n",
    "\t\t\tpass\n",
    "\treturn count\n",
    "\n",
    "# num=image_augument(1,\"child\",\"male\")\n",
    "# initial_face(num,\"child\",\"male\")\n",
    "\n",
    "\n",
    "# def img2csv (age, gender):\n",
    "# \tbmpfile=glob.glob(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'+'*.jpg')\n",
    "# \tX_train = []\n",
    "# \tfor bmp_dir in bmpfile:\n",
    "# \t\timg = cv2.imread(bmp_dir,0)\n",
    "# \t\tflatten_img = np.array(img).flatten() # a = np.array([1,2],[3,4]) , a.T.flatten [1,3,2,4] , a.flatten [1,2,3,4]\n",
    "# \t\t#X_train = np.append(X_train, flatten_img, axis = 0)\n",
    "# \t\tX_train.append(flatten_img)\n",
    "# \tX_train = np.array(X_train)\n",
    "# \tnp.savetxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'_'+gender+\".csv\", X_train, delimiter=',')\n",
    "# \treturn X_train\n",
    "\n",
    "# def merge_train_data():\n",
    "# \ttry:\n",
    "# \t\tos.remove(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\")\n",
    "# \texcept OSError:\n",
    "# \t\tpass\n",
    "# \tcsvs=glob.glob('/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/*.csv')\n",
    "# \ttrain_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\",\"a\")\n",
    "# \tlable_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_train.csv\",\"w\")\n",
    "# \tcount = 0\n",
    "# \tY_train = 0\n",
    "# \tfor csv_file in csvs:\n",
    "# \t\tf = open(csv_file)\n",
    "# \t\tfor line in f:\n",
    "# \t\t\tY_train += 1\n",
    "# \t\t\ttrain_data.write(line)\n",
    "# \t\t\tlable_data.write(str(count))\n",
    "# \t\t\tlable_data.write('\\n')\n",
    "# \t\tcount +=1\n",
    "# \t\tf.close()\n",
    "# \ttrain_data.close()\n",
    "# \tlable_data.close()\n",
    "# \t#return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_image = image_augument(1210,'adult','female')\n",
    "# num_eff_facedetection = initial_face(num_image,'adult','female')\n",
    "# print (\"adult female num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(1588,'adult','male')\n",
    "# num_eff_facedetection = initial_face(num_image,'adult','male')\n",
    "# print (\"adult male num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(774,'child','female')\n",
    "# num_eff_facedetection = initial_face(num_image,'child','female')\n",
    "# print (\"child female num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(650,'child','male')\n",
    "# num_eff_facedetection = initial_face(num_image,'child','male')\n",
    "# print (\"child male num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(452,'elder','female')\n",
    "# num_eff_facedetection = initial_face(num_image,'elder','female')\n",
    "# print (\"elder female num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(878,'elder','male')\n",
    "# num_eff_facedetection = initial_face(num_image,'elder','male')\n",
    "# print (\"elder male num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(1002,'young','female')\n",
    "# num_eff_facedetection = initial_face(num_image,'young','female')\n",
    "# print (\"young female num_eff_facedetection\",num_eff_facedetection)\n",
    "\n",
    "# num_image = image_augument(830,'young','male')\n",
    "# num_eff_facedetection = initial_face(num_image,'young','male')\n",
    "# print (\"young male num_eff_facedetection\",num_eff_facedetection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15105, 30, 30)\n",
      "(15105,)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "def img2csv (num_eff_facedetection, age, gender):\n",
    "\tbmpfile=glob.glob(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'+'*.jpg')\n",
    "\tX_train = np.zeros((num_eff_facedetection,30,30))\n",
    "\tcount = 0\n",
    "\tfor bmp_dir in bmpfile:\n",
    "\t\timg = cv2.imread(bmp_dir,0)\n",
    "\t\timg = np.array(img)\n",
    "\t\tX_train[count,:,:] = img[:,:]\n",
    "\t\tcount += 1\n",
    "\treturn X_train\n",
    "\n",
    "X_train_0 = img2csv(2685,'adult','female')\n",
    "X_train_1 = img2csv(2572,'adult','male')\n",
    "X_train_2 = img2csv(1839,'child','female')\n",
    "X_train_3 = img2csv(1542,'child','male')\n",
    "X_train_4 = img2csv(1102,'elder','female')\n",
    "X_train_5 = img2csv(1427,'elder','male')\n",
    "X_train_6 = img2csv(2066,'young','female')\n",
    "X_train_7 = img2csv(1872,'young','male')\n",
    "X_train = np.concatenate((X_train_0, X_train_1, X_train_2, X_train_3, X_train_4, X_train_5, X_train_6, X_train_7),axis = 0)\n",
    "Y_train = np.concatenate((np.zeros(2685),np.ones(2572),2*np.ones(1839),3*np.ones(1542),4*np.ones(1102),5*np.ones(1427),6*np.ones(2066),7*np.ones(1872)),axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15105 samples, validate on 15105 samples\n",
      "Epoch 1/100\n",
      "15105/15105 [==============================] - 48s - loss: 2.2246 - acc: 0.1615 - val_loss: 2.1419 - val_acc: 0.1782\n",
      "Epoch 2/100\n",
      "15105/15105 [==============================] - 43s - loss: 2.0475 - acc: 0.1979 - val_loss: 2.0283 - val_acc: 0.2075\n",
      "Epoch 3/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.9781 - acc: 0.2283 - val_loss: 1.9106 - val_acc: 0.2448\n",
      "Epoch 4/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.8823 - acc: 0.2526 - val_loss: 1.9948 - val_acc: 0.1954\n",
      "Epoch 5/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.8064 - acc: 0.2806 - val_loss: 1.9502 - val_acc: 0.2352\n",
      "Epoch 6/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.7373 - acc: 0.3105 - val_loss: 1.6715 - val_acc: 0.3533\n",
      "Epoch 7/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.6740 - acc: 0.3439 - val_loss: 1.8857 - val_acc: 0.3043\n",
      "Epoch 8/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.6050 - acc: 0.3746 - val_loss: 1.7274 - val_acc: 0.3270\n",
      "Epoch 9/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.5586 - acc: 0.3929 - val_loss: 1.5293 - val_acc: 0.3898\n",
      "Epoch 10/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.5103 - acc: 0.4098 - val_loss: 1.5588 - val_acc: 0.3846\n",
      "Epoch 11/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.4797 - acc: 0.4211 - val_loss: 1.9980 - val_acc: 0.2873\n",
      "Epoch 12/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.4175 - acc: 0.4454 - val_loss: 2.0784 - val_acc: 0.3217\n",
      "Epoch 13/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.3948 - acc: 0.4495 - val_loss: 1.7205 - val_acc: 0.3405\n",
      "Epoch 14/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.3543 - acc: 0.4719 - val_loss: 1.3128 - val_acc: 0.4824\n",
      "Epoch 15/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.3266 - acc: 0.4774 - val_loss: 2.1637 - val_acc: 0.2694\n",
      "Epoch 16/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.2946 - acc: 0.4909 - val_loss: 1.6939 - val_acc: 0.3765\n",
      "Epoch 17/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.2575 - acc: 0.5074 - val_loss: 1.6647 - val_acc: 0.3741\n",
      "Epoch 18/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.2114 - acc: 0.5247 - val_loss: 1.2375 - val_acc: 0.5200\n",
      "Epoch 19/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.1973 - acc: 0.5294 - val_loss: 1.5855 - val_acc: 0.3787\n",
      "Epoch 20/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.1515 - acc: 0.5466 - val_loss: 1.0545 - val_acc: 0.5834\n",
      "Epoch 21/100\n",
      "15105/15105 [==============================] - 40s - loss: 1.1325 - acc: 0.5539 - val_loss: 1.6773 - val_acc: 0.4169\n",
      "Epoch 22/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.0813 - acc: 0.5793 - val_loss: 1.1031 - val_acc: 0.5637\n",
      "Epoch 23/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.0594 - acc: 0.5824 - val_loss: 1.3221 - val_acc: 0.4871\n",
      "Epoch 24/100\n",
      "15105/15105 [==============================] - 41s - loss: 1.0305 - acc: 0.5928 - val_loss: 1.3145 - val_acc: 0.5054\n",
      "Epoch 25/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.9937 - acc: 0.6095 - val_loss: 1.7349 - val_acc: 0.4428\n",
      "Epoch 26/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.9469 - acc: 0.6303 - val_loss: 1.0035 - val_acc: 0.6013\n",
      "Epoch 27/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.9541 - acc: 0.6234 - val_loss: 1.2530 - val_acc: 0.5288\n",
      "Epoch 28/100\n",
      "15105/15105 [==============================] - 39s - loss: 0.9215 - acc: 0.6354 - val_loss: 0.9067 - val_acc: 0.6429\n",
      "Epoch 29/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.8900 - acc: 0.6514 - val_loss: 0.7683 - val_acc: 0.7016\n",
      "Epoch 30/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.8778 - acc: 0.6593 - val_loss: 0.7487 - val_acc: 0.7057\n",
      "Epoch 31/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.8562 - acc: 0.6677 - val_loss: 0.9239 - val_acc: 0.6435\n",
      "Epoch 32/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.8253 - acc: 0.6762 - val_loss: 1.3779 - val_acc: 0.5178\n",
      "Epoch 33/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.8002 - acc: 0.6895 - val_loss: 0.8165 - val_acc: 0.6773\n",
      "Epoch 34/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.7713 - acc: 0.6982 - val_loss: 0.9484 - val_acc: 0.6642\n",
      "Epoch 35/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.7611 - acc: 0.7051 - val_loss: 1.3769 - val_acc: 0.5253\n",
      "Epoch 36/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.7235 - acc: 0.7179 - val_loss: 0.8759 - val_acc: 0.6645\n",
      "Epoch 37/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.7047 - acc: 0.7245 - val_loss: 1.8036 - val_acc: 0.4278\n",
      "Epoch 38/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.6578 - acc: 0.7461 - val_loss: 0.6237 - val_acc: 0.7590\n",
      "Epoch 39/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.6642 - acc: 0.7420 - val_loss: 1.9856 - val_acc: 0.4651\n",
      "Epoch 40/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.6322 - acc: 0.7582 - val_loss: 0.8176 - val_acc: 0.6926\n",
      "Epoch 41/100\n",
      "15105/15105 [==============================] - 47s - loss: 0.6218 - acc: 0.7605 - val_loss: 1.1678 - val_acc: 0.5819\n",
      "Epoch 42/100\n",
      "15105/15105 [==============================] - 42s - loss: 0.5919 - acc: 0.7767 - val_loss: 0.4593 - val_acc: 0.8269\n",
      "Epoch 43/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.5936 - acc: 0.7747 - val_loss: 0.8342 - val_acc: 0.6965\n",
      "Epoch 44/100\n",
      "15105/15105 [==============================] - 40s - loss: 0.5704 - acc: 0.7842 - val_loss: 0.9311 - val_acc: 0.6577\n",
      "Epoch 45/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.5570 - acc: 0.7883 - val_loss: 0.4450 - val_acc: 0.8283\n",
      "Epoch 46/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.5639 - acc: 0.7871 - val_loss: 1.2175 - val_acc: 0.5985\n",
      "Epoch 47/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.5422 - acc: 0.7927 - val_loss: 0.4016 - val_acc: 0.8470\n",
      "Epoch 48/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.5278 - acc: 0.8030 - val_loss: 0.9259 - val_acc: 0.6583\n",
      "Epoch 49/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.4966 - acc: 0.8098 - val_loss: 0.4073 - val_acc: 0.8440\n",
      "Epoch 50/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.4861 - acc: 0.8189 - val_loss: 0.3760 - val_acc: 0.8600\n",
      "Epoch 51/100\n",
      "15105/15105 [==============================] - 41s - loss: 0.4831 - acc: 0.8197 - val_loss: 1.5414 - val_acc: 0.5611\n",
      "Epoch 52/100\n",
      "15105/15105 [==============================] - 44s - loss: 0.4556 - acc: 0.8330 - val_loss: 0.8061 - val_acc: 0.7228\n",
      "Epoch 53/100\n",
      "15105/15105 [==============================] - 45s - loss: 0.4533 - acc: 0.8342 - val_loss: 0.4542 - val_acc: 0.8226\n",
      "Epoch 54/100\n",
      "15105/15105 [==============================] - 43s - loss: 0.4495 - acc: 0.8350 - val_loss: 0.7677 - val_acc: 0.7483\n",
      "Epoch 55/100\n",
      " 5312/15105 [=========>....................] - ETA: 23s - loss: 0.4302 - acc: 0.8470"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from numpy import genfromtxt\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes=8)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 8\n",
    "epochs = 100\n",
    "img_rows, img_cols = 30, 30\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    #X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    #X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(7, 7),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(X_train, Y_train))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_train, Y_train))\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_testing.csv\",delimiter=',')\n",
    "Y_test = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_testing.csv\",delimiter=',')\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes=8)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('/Users/jack_wu/IMG_2685.JPG')\n",
    "# output_address = \"/Users/jack_wu/\"\n",
    "# faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "# s_jpg='.jpg'\n",
    "\n",
    "# if (len(faces)!=0):\n",
    "# \tfor (x,y,w,h) in faces:\n",
    "# \t\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# \t\tcv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "# \t\troi_gray = gray[y:y+h, x:x+w]\n",
    "# \t\tresize_face=cv2.resize(roi_gray,(30,30),interpolation=cv2.INTER_AREA)\n",
    "# \t\tcv2.imwrite(output_address+'test'+s_jpg, resize_face, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "\n",
    "# img = cv2.imread('/Users/jack_wu/test.JPG',0)\n",
    "# img = np.array(img)\n",
    "# img_rows = img.shape[0]\n",
    "# img_cols = img.shape[1]\n",
    "# test_img = np.zeros((1, img_rows, img_cols))\n",
    "# test_img [0,:,:] = img[:,:]\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     test_img = test_img.reshape(test_img.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     test_img = test_img.reshape(test_img.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# ans_img = np.array(7)\n",
    "# ans_img = np_utils.to_categorical(ans_img, num_classes=8)\n",
    "\n",
    "# score = model.evaluate(test_img, ans_img, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
