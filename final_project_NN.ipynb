{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "################################# initial ########################################\n",
    "face_size = 30\n",
    "################################# Path ########################################\n",
    "face_cascade = cv2.CascadeClassifier('/Users/jack_wu/opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('/Users/jack_wu/opencv/data/haarcascades/haarcascade_eye.xml')\n",
    "#img = cv2.imread('/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/FinalProject_dataset/adult/female/183.jpg')\n",
    "#img = cv2.imread('0.JPG')\n",
    "\n",
    "\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "# for (x,y,w,h) in faces:\n",
    "#     cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#     roi_gray = gray[y:y+h, x:x+w]\n",
    "#     roi_color = img[y:y+h, x:x+w]\n",
    "#     eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "#     for (ex,ey,ew,eh) in eyes:\n",
    "#         cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "# cv2.imshow('img',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_face(item_Num, age, gender):\n",
    "\ts_jpg='.jpg'\n",
    "\tcount = 0\n",
    "\tinput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/FinalProject_dataset/\"+age+'/'+gender+'/'\n",
    "\toutput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'\n",
    "\tfor file_count in range(item_Num):\n",
    "\t\ts_file_count = str(file_count)\n",
    "\t\timg = cv2.imread(input_address + s_file_count + s_jpg)\n",
    "\t\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\t\t\n",
    "\t\tif (len(faces)!=0):\n",
    "\t\t\tfor (x,y,w,h) in faces:\n",
    "\t\t\t\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\t\t\t\troi_gray = gray[y:y+h, x:x+w]\n",
    "\t\t\t\tresize_face=cv2.resize(roi_gray,(30,30),interpolation=cv2.INTER_AREA)\n",
    "\t\t\t\tcv2.imwrite(output_address+str(count)+s_jpg, resize_face, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\telse:\n",
    "\t\t\t## No face detect\n",
    "\t\t\tpass\n",
    "\treturn count\n",
    "\n",
    "def horizonal_filpping(item_Num, age, gender): #item_Num = count\n",
    "\ts_jpg='.jpg'\n",
    "\tcount = 0\n",
    "\tinput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'\n",
    "\toutput_address = \"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'\n",
    "\tfor file_count in range(item_Num):\n",
    "\t\ts_file_count = str(file_count)\n",
    "\t\timg = cv2.imread(input_address + s_file_count + s_jpg)\n",
    "\t\tfilp_img = cv2.flip( img, 1)\n",
    "\t\tcv2.imwrite(output_address+str(item_Num+count)+s_jpg, filp_img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "\t\tcount = count +1\n",
    "\treturn count\n",
    "\n",
    "def img2csv (age, gender):\n",
    "\tbmpfile=glob.glob(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'/'+gender+'/'+'*.jpg')\n",
    "\tX_train = []\n",
    "\tfor bmp_dir in bmpfile:\n",
    "\t\timg = cv2.imread(bmp_dir,0)\n",
    "\t\tflatten_img = np.array(img).flatten() # a = np.array([1,2],[3,4]) , a.T.flatten [1,3,2,4] , a.flatten [1,2,3,4]\n",
    "\t\t#X_train = np.append(X_train, flatten_img, axis = 0)\n",
    "\t\tX_train.append(flatten_img)\n",
    "\tX_train = np.array(X_train)\n",
    "\tnp.savetxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/\"+age+'_'+gender+\".csv\", X_train, delimiter=',')\n",
    "\treturn X_train\n",
    "\n",
    "def merge_train_data():\n",
    "\ttry:\n",
    "\t\tos.remove(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\")\n",
    "\texcept OSError:\n",
    "\t\tpass\n",
    "\tcsvs=glob.glob('/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Face_detection/*.csv')\n",
    "\ttrain_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\",\"a\")\n",
    "\tlable_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_train.csv\",\"w\")\n",
    "\tcount = 0\n",
    "\tY_train = 0\n",
    "\tfor csv_file in csvs:\n",
    "\t\tf = open(csv_file)\n",
    "\t\tfor line in f:\n",
    "\t\t\tY_train += 1\n",
    "\t\t\ttrain_data.write(line)\n",
    "\t\t\tlable_data.write(str(count))\n",
    "\t\t\tlable_data.write('\\n')\n",
    "\t\tcount +=1\n",
    "\t\tf.close()\n",
    "\ttrain_data.close()\n",
    "\tlable_data.close()\n",
    "\t#return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res=cv2.resize(face,(30,30),interpolation=cv2.INTER_CUBIC)\n",
    "# cv2.imshow('img',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1)\n",
    "\n",
    "# adult_female_count = initial_face(1210,'adult','female')\n",
    "# flip_img = horizonal_filpping(adult_female_count,'adult','female')\n",
    "# X_train_shape=img2csv ('adult','female')\n",
    "# print (\"Train size of adult_female\",X_train_shape.shape)\n",
    "\n",
    "# adult_male_count = initial_face(1588,'adult','male')\n",
    "# flip_img = horizonal_filpping(adult_male_count,'adult','male')\n",
    "# X_train_shape=img2csv ('adult','male')\n",
    "# print (\"Train size of adult_male\",X_train_shape.shape)\n",
    "\n",
    "# child_female_count = initial_face(774,'child','female')\n",
    "# flip_img = horizonal_filpping(child_female_count,'child','female')\n",
    "# X_train_shape=img2csv ('child','female')\n",
    "# print (\"Train size of child_female\",X_train_shape.shape)\n",
    "\n",
    "# child_male_count = initial_face(650,'child','male')\n",
    "# flip_img = horizonal_filpping(child_male_count,'child','male')\n",
    "# X_train_shape=img2csv ('child','male')\n",
    "# print (\"Train size of child_male\",X_train_shape.shape)\n",
    "\n",
    "# elder_female_count = initial_face(452,'elder','female')\n",
    "# flip_img = horizonal_filpping(elder_female_count,'elder','female')\n",
    "# X_train_shape=img2csv ('elder','female')\n",
    "# print (\"Train size of elder_female\",X_train_shape.shape)\n",
    "\n",
    "# elder_male_count = initial_face(878,'elder','male')\n",
    "# flip_img = horizonal_filpping(elder_male_count,'elder','male')\n",
    "# X_train_shape=img2csv ('elder','male')\n",
    "# print (\"Train size of elder_male\",X_train_shape.shape)\n",
    "\n",
    "# young_female_count = initial_face(1002,'young','female')\n",
    "# flip_img = horizonal_filpping(young_female_count,'young','female')\n",
    "# X_train_shape=img2csv ('young','female')\n",
    "# print (\"Train size of young_female\",X_train_shape.shape)\n",
    "\n",
    "# young_male_count = initial_face(830,'young','male')\n",
    "# flip_img = horizonal_filpping(young_male_count,'young','male')\n",
    "# X_train_shape=img2csv ('young','male')\n",
    "# print (\"Train size of young_male\",X_train_shape.shape)\n",
    "\n",
    "\n",
    "\n",
    "#adult-> female, num_pic = 1210, effective_pic = 1069\n",
    "#adult-> male, num_pic = 1588, effective_pic = 1268\n",
    "#child-> female, num_pic = 774, effective_pic = 626\n",
    "#child-> male, num_pic = 650, effective_pic = 510\n",
    "#elder-> female, num_pic = 452, effective_pic = 351\n",
    "#elder-> male, num_pic = 878, effective_pic = 649\n",
    "#young-> female, num_pic = 1002, effective_pic = 729\n",
    "#young-> male, num_pic = 830, effective_pic = 670\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import random\n",
    "def merge_testing_data(percentage):\n",
    "\n",
    "\t#train_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\",\"r\")\n",
    "\t#lable_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_train.csv\",\"r\")\n",
    "\t\n",
    "\ttrain_data = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\",delimiter=',')\n",
    "\tlable_data = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_train.csv\",delimiter=',')\n",
    "\t\n",
    "\tnum_testing = len(train_data) * percentage // 100\n",
    "\tindicies = random.sample(range(len(train_data)), num_testing)\n",
    "\t\n",
    "\t\n",
    "\t#f2=open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/hw2/final/d_w_total.csv\",'w')\n",
    "\t#w_file2=csv.writer(f2)\n",
    "\t#w_file2.writerows(w_total)\n",
    "\n",
    "\twith open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_testing.csv\",'w') as X_testing:\n",
    "\t\tfor i in indicies:\n",
    "\t\t\ttest_data = csv.writer(X_testing, dialect='excel')\n",
    "\t\t\ttest_data.writerow(train_data[i])\n",
    "\t\t\t\n",
    "\ttest_lable_data = open(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_testing.csv\",\"w\")\n",
    "\tfor i in indicies:\n",
    "\t\ttest_lable_data.write(str(lable_data[i]))\n",
    "\t\ttest_lable_data.write('\\n')\n",
    "\n",
    "\ttest_lable_data.close()\n",
    "\n",
    "#merge_train_data()\n",
    "merge_testing_data(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (15105, 900)\n",
      "Y_train (15105, 8)\n",
      "X_test (3021, 900)\n",
      "Y_test (3021, 8)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from numpy import genfromtxt\n",
    "import random\n",
    "\n",
    "X_train = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_train.csv\",delimiter=',')\n",
    "Y_train = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_train.csv\",delimiter=',')\n",
    "\n",
    "X_test = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/X_testing.csv\",delimiter=',')\n",
    "Y_test = genfromtxt(\"/Users/jack_wu/Documents/Machine learing/Lecture/ML/final/Y_testing.csv\",delimiter=',')\n",
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.   # normalize\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes=8)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.   # normalize\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes=8)\n",
    "\n",
    "print (\"X train\",X_train.shape)\n",
    "print (\"Y_train\",Y_train.shape)\n",
    "print (\"X_test\",X_test.shape)\n",
    "print (\"Y_test\",Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ------------\n",
      "Epoch 1/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.9212 - acc: 0.2508     \n",
      "Epoch 2/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.8237 - acc: 0.2969     \n",
      "Epoch 3/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.7800 - acc: 0.3172     \n",
      "Epoch 4/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.7488 - acc: 0.3376     \n",
      "Epoch 5/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.7223 - acc: 0.3476     \n",
      "Epoch 6/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.7105 - acc: 0.3499     \n",
      "Epoch 7/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6906 - acc: 0.3613     \n",
      "Epoch 8/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6762 - acc: 0.3660     \n",
      "Epoch 9/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6632 - acc: 0.3717     \n",
      "Epoch 10/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6563 - acc: 0.3725     \n",
      "Epoch 11/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6429 - acc: 0.3789     \n",
      "Epoch 12/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6392 - acc: 0.3847     \n",
      "Epoch 13/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6246 - acc: 0.3846     \n",
      "Epoch 14/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6176 - acc: 0.3859     \n",
      "Epoch 15/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6073 - acc: 0.3948     \n",
      "Epoch 16/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6003 - acc: 0.3925     \n",
      "Epoch 17/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.6085 - acc: 0.3950     \n",
      "Epoch 18/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5916 - acc: 0.3972     \n",
      "Epoch 19/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5964 - acc: 0.3954     \n",
      "Epoch 20/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5860 - acc: 0.4018     \n",
      "Epoch 21/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5742 - acc: 0.4042     \n",
      "Epoch 22/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5772 - acc: 0.4041     \n",
      "Epoch 23/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5621 - acc: 0.4140     \n",
      "Epoch 24/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5658 - acc: 0.4095     \n",
      "Epoch 25/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5582 - acc: 0.4131     \n",
      "Epoch 26/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5608 - acc: 0.4107     \n",
      "Epoch 27/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5579 - acc: 0.4134     \n",
      "Epoch 28/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5619 - acc: 0.4068     \n",
      "Epoch 29/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5568 - acc: 0.4117     \n",
      "Epoch 30/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5734 - acc: 0.4015     \n",
      "Epoch 31/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5663 - acc: 0.4036     \n",
      "Epoch 32/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5656 - acc: 0.4066     \n",
      "Epoch 33/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5428 - acc: 0.4132     \n",
      "Epoch 34/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5436 - acc: 0.4140     \n",
      "Epoch 35/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5391 - acc: 0.4181     \n",
      "Epoch 36/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5354 - acc: 0.4194     \n",
      "Epoch 37/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5337 - acc: 0.4169     \n",
      "Epoch 38/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5347 - acc: 0.4191     \n",
      "Epoch 39/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5268 - acc: 0.4228     \n",
      "Epoch 40/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5275 - acc: 0.4280     \n",
      "Epoch 41/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5238 - acc: 0.4218     \n",
      "Epoch 42/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5161 - acc: 0.4223     \n",
      "Epoch 43/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5197 - acc: 0.4260     \n",
      "Epoch 44/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5106 - acc: 0.4280     \n",
      "Epoch 45/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5148 - acc: 0.4284     \n",
      "Epoch 46/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5085 - acc: 0.4293     \n",
      "Epoch 47/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5077 - acc: 0.4325     \n",
      "Epoch 48/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5161 - acc: 0.4289     \n",
      "Epoch 49/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5010 - acc: 0.4288     \n",
      "Epoch 50/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4915 - acc: 0.4374     \n",
      "Epoch 51/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4923 - acc: 0.4372     \n",
      "Epoch 52/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.5040 - acc: 0.4338     \n",
      "Epoch 53/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4898 - acc: 0.4362     - ETA: 1s - los\n",
      "Epoch 54/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4971 - acc: 0.4314     \n",
      "Epoch 55/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4823 - acc: 0.4405     \n",
      "Epoch 56/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4840 - acc: 0.4404     \n",
      "Epoch 57/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4802 - acc: 0.4430     \n",
      "Epoch 58/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4875 - acc: 0.4391     \n",
      "Epoch 59/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4848 - acc: 0.4351     \n",
      "Epoch 60/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4732 - acc: 0.4384     \n",
      "Epoch 61/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4743 - acc: 0.4455     \n",
      "Epoch 62/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4725 - acc: 0.4445     \n",
      "Epoch 63/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4799 - acc: 0.4407     \n",
      "Epoch 64/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4808 - acc: 0.4399     \n",
      "Epoch 65/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4696 - acc: 0.4461     \n",
      "Epoch 66/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4603 - acc: 0.4478     \n",
      "Epoch 67/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4634 - acc: 0.4510     \n",
      "Epoch 68/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4666 - acc: 0.4458     \n",
      "Epoch 69/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4622 - acc: 0.4480     \n",
      "Epoch 70/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4592 - acc: 0.4508     \n",
      "Epoch 71/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4599 - acc: 0.4489     \n",
      "Epoch 72/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4555 - acc: 0.4502     \n",
      "Epoch 73/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4674 - acc: 0.4465     \n",
      "Epoch 74/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4520 - acc: 0.4536     \n",
      "Epoch 75/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4612 - acc: 0.4506     \n",
      "Epoch 76/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4500 - acc: 0.4526     \n",
      "Epoch 77/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4516 - acc: 0.4549     \n",
      "Epoch 78/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4949 - acc: 0.4348     \n",
      "Epoch 79/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4561 - acc: 0.4485     \n",
      "Epoch 80/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4479 - acc: 0.4522     \n",
      "Epoch 81/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4437 - acc: 0.4578     \n",
      "Epoch 82/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4510 - acc: 0.4527     \n",
      "Epoch 83/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4387 - acc: 0.4580     \n",
      "Epoch 84/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4416 - acc: 0.4548     \n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15105/15105 [==============================] - 1s - loss: 1.4482 - acc: 0.4553     \n",
      "Epoch 86/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4364 - acc: 0.4608     \n",
      "Epoch 87/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4299 - acc: 0.4549     \n",
      "Epoch 88/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4299 - acc: 0.4615     \n",
      "Epoch 89/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4359 - acc: 0.4536     \n",
      "Epoch 90/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4355 - acc: 0.4559     \n",
      "Epoch 91/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4399 - acc: 0.4589     \n",
      "Epoch 92/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4330 - acc: 0.4589     \n",
      "Epoch 93/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4267 - acc: 0.4621     \n",
      "Epoch 94/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4250 - acc: 0.4600     \n",
      "Epoch 95/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4214 - acc: 0.4631     \n",
      "Epoch 96/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4305 - acc: 0.4612     \n",
      "Epoch 97/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4240 - acc: 0.4669     \n",
      "Epoch 98/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4224 - acc: 0.4601     \n",
      "Epoch 99/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4174 - acc: 0.4610     \n",
      "Epoch 100/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4192 - acc: 0.4639     \n",
      "Epoch 101/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4173 - acc: 0.4655     \n",
      "Epoch 102/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4173 - acc: 0.4640     \n",
      "Epoch 103/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3999 - acc: 0.4732     \n",
      "Epoch 104/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4106 - acc: 0.4651     \n",
      "Epoch 105/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4206 - acc: 0.4612     \n",
      "Epoch 106/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4116 - acc: 0.4667     \n",
      "Epoch 107/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4063 - acc: 0.4693     \n",
      "Epoch 108/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4158 - acc: 0.4640     \n",
      "Epoch 109/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4105 - acc: 0.4669     \n",
      "Epoch 110/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4029 - acc: 0.4673     \n",
      "Epoch 111/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4069 - acc: 0.4685     \n",
      "Epoch 112/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4001 - acc: 0.4716     \n",
      "Epoch 113/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4059 - acc: 0.4667     \n",
      "Epoch 114/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3987 - acc: 0.4744     \n",
      "Epoch 115/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3987 - acc: 0.4747     \n",
      "Epoch 116/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4036 - acc: 0.4728     \n",
      "Epoch 117/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3973 - acc: 0.4726     \n",
      "Epoch 118/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3958 - acc: 0.4701     \n",
      "Epoch 119/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4100 - acc: 0.4649     \n",
      "Epoch 120/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3896 - acc: 0.4731     \n",
      "Epoch 121/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3961 - acc: 0.4734     \n",
      "Epoch 122/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3886 - acc: 0.4720     \n",
      "Epoch 123/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3876 - acc: 0.4748     \n",
      "Epoch 124/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3872 - acc: 0.4761     \n",
      "Epoch 125/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3910 - acc: 0.4757     \n",
      "Epoch 126/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3888 - acc: 0.4759     \n",
      "Epoch 127/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3839 - acc: 0.4776     \n",
      "Epoch 128/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3880 - acc: 0.4775     \n",
      "Epoch 129/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3894 - acc: 0.4783     \n",
      "Epoch 130/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3841 - acc: 0.4727     \n",
      "Epoch 131/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.4025 - acc: 0.4728     \n",
      "Epoch 132/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3834 - acc: 0.4799     \n",
      "Epoch 133/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3859 - acc: 0.4771     \n",
      "Epoch 134/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3779 - acc: 0.4795     \n",
      "Epoch 135/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3848 - acc: 0.4757     \n",
      "Epoch 136/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3749 - acc: 0.4818     \n",
      "Epoch 137/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3781 - acc: 0.4787     \n",
      "Epoch 138/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3765 - acc: 0.4829     \n",
      "Epoch 139/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3748 - acc: 0.4820     \n",
      "Epoch 140/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3829 - acc: 0.4745     \n",
      "Epoch 141/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3679 - acc: 0.4803     \n",
      "Epoch 142/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3694 - acc: 0.4815     \n",
      "Epoch 143/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3688 - acc: 0.4794     \n",
      "Epoch 144/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3737 - acc: 0.4788     \n",
      "Epoch 145/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3829 - acc: 0.4770     \n",
      "Epoch 146/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3686 - acc: 0.4803     \n",
      "Epoch 147/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3710 - acc: 0.4829     \n",
      "Epoch 148/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3695 - acc: 0.4777     \n",
      "Epoch 149/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3609 - acc: 0.4846     \n",
      "Epoch 150/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3715 - acc: 0.4765     \n",
      "Epoch 151/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3637 - acc: 0.4806     \n",
      "Epoch 152/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3731 - acc: 0.4824     \n",
      "Epoch 153/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3632 - acc: 0.4836     \n",
      "Epoch 154/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3818 - acc: 0.4794     \n",
      "Epoch 155/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3584 - acc: 0.4846     \n",
      "Epoch 156/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3560 - acc: 0.4888     \n",
      "Epoch 157/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3646 - acc: 0.4818     \n",
      "Epoch 158/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3548 - acc: 0.4853     \n",
      "Epoch 159/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3545 - acc: 0.4861     \n",
      "Epoch 160/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3634 - acc: 0.4841     \n",
      "Epoch 161/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3500 - acc: 0.4879     \n",
      "Epoch 162/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3562 - acc: 0.4864     \n",
      "Epoch 163/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3573 - acc: 0.4867     \n",
      "Epoch 164/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3533 - acc: 0.4840     \n",
      "Epoch 165/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3539 - acc: 0.4877     \n",
      "Epoch 166/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3502 - acc: 0.4888     \n",
      "Epoch 167/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3437 - acc: 0.4939     \n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15105/15105 [==============================] - 1s - loss: 1.3617 - acc: 0.4800     \n",
      "Epoch 169/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3512 - acc: 0.4916     \n",
      "Epoch 170/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3581 - acc: 0.4863     \n",
      "Epoch 171/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3427 - acc: 0.4916     \n",
      "Epoch 172/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3467 - acc: 0.4895     \n",
      "Epoch 173/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3488 - acc: 0.4876     \n",
      "Epoch 174/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3440 - acc: 0.4929     \n",
      "Epoch 175/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3492 - acc: 0.4915     \n",
      "Epoch 176/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3479 - acc: 0.4861     \n",
      "Epoch 177/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3419 - acc: 0.4922     \n",
      "Epoch 178/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3464 - acc: 0.4897     \n",
      "Epoch 179/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3422 - acc: 0.4916     \n",
      "Epoch 180/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3558 - acc: 0.4853     \n",
      "Epoch 181/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3683 - acc: 0.4829     \n",
      "Epoch 182/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3474 - acc: 0.4861     \n",
      "Epoch 183/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3365 - acc: 0.4907     \n",
      "Epoch 184/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3459 - acc: 0.4916     \n",
      "Epoch 185/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3337 - acc: 0.4967     \n",
      "Epoch 186/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3412 - acc: 0.4914     \n",
      "Epoch 187/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3506 - acc: 0.4843     \n",
      "Epoch 188/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3266 - acc: 0.4941     \n",
      "Epoch 189/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3376 - acc: 0.4908     \n",
      "Epoch 190/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3384 - acc: 0.4937     \n",
      "Epoch 191/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3400 - acc: 0.4871     \n",
      "Epoch 192/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3441 - acc: 0.4920     \n",
      "Epoch 193/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3336 - acc: 0.4928     \n",
      "Epoch 194/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3261 - acc: 0.4961     \n",
      "Epoch 195/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3298 - acc: 0.4938     \n",
      "Epoch 196/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3355 - acc: 0.4940     \n",
      "Epoch 197/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3334 - acc: 0.4926     \n",
      "Epoch 198/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3309 - acc: 0.4984     \n",
      "Epoch 199/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3318 - acc: 0.4949     \n",
      "Epoch 200/200\n",
      "15105/15105 [==============================] - 1s - loss: 1.3261 - acc: 0.4974     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a765b00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=900),\n",
    "    Activation('relu'),\n",
    "    Dense(8),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "# Another way to define your optimizer\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# We add metrics to get more results you want to see\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training ------------')\n",
    "# Another way to train the model\n",
    "model.fit(X_train, Y_train, epochs=200, shuffle=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "1696/3021 [===============>..............] - ETA: 0stest loss:  1.33250488993\n",
      "test accuracy:  0.489241972886\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "# Evaluate the model with the metrics we defined earlier\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
